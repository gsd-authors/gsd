{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GSD","text":"<p>Reference implementation of generalised score distribution in python</p> <p>This library provides a reference implementation of gsd probabilities for correctness and efficient implementation of samples and log_probabilities in <code>jax</code>.</p>"},{"location":"#citations","title":"Citations","text":"<p>Theoretical derivation of GSD is described in the following papers.</p> <pre><code>@Article{Cmiel2023,\nauthor={{\\'{C}}miel, Bogdan\nand Nawa{\\l}a, Jakub\nand Janowski, Lucjan\nand Rusek, Krzysztof},\ntitle={Generalised score distribution: underdispersed continuation of the beta-binomial distribution},\njournal={Statistical Papers},\nyear={2023},\nmonth={Feb},\nday={09},\nissn={1613-9798},\ndoi={10.1007/s00362-023-01398-0},\nurl={https://doi.org/10.1007/s00362-023-01398-0}\n}\n</code></pre> <pre><code>@ARTICLE{gsdnawala,\n  author={Nawa\u0142a, Jakub and Janowski, Lucjan and \u0106miel, Bogdan and Rusek, Krzysztof and P\u00e9rez, Pablo},\n  journal={IEEE Transactions on Multimedia}, \n  title={Generalized Score Distribution: A Two-Parameter Discrete Distribution Accurately Describing Responses From Quality of Experience Subjective Experiments}, \n  year={2022},\n  volume={},\n  number={},\n  pages={1-15},\n  doi={10.1109/TMM.2022.3205444}\n  }\n</code></pre> <p>If you decide to apply the concepts presented or base on the provided code, please do refer our related paper.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install gsd via <code>pip</code>:</p> <pre><code>pip install ref_gsd\n</code></pre> <p>Note that you install <code>ref_gsd</code> but import <code>gsd</code> e.g.</p> <pre><code>import gsd\n\ngsd.experimental.fit_moments([2, 8, 2, 0, 0.])\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>To develop and modify gsd, you need to install <code>hatch</code>, a tool for Python packaging and dependency management.</p> <p>To  enter a virtual environment for testing or debugging, you can run:</p> <pre><code>hatch shell\n</code></pre>"},{"location":"#running-tests","title":"Running tests","text":"<p>Gsd uses unitest for testing. To run the tests, use the following command:</p> <pre><code>hatch run test \n</code></pre>"},{"location":"#standalone-estimator","title":"Standalone estimator","text":"<p>You can quickly estimate GSD parameters from a command line interface</p> <pre><code>python3 -m gsd -c 1 2 3 4 5\n</code></pre> <pre><code>psi=3.6667 rho=0.6000\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#reference-implementation-in-python","title":"Reference implementation in python","text":"<p>In order to keep the reference implementation as close to the math as possible we define some utilities with unicode symbols. E.g.  <code>\ud835\udeb7(i for i in \u2124[1,3])</code> is a valid python code for </p> <p>\\(\\prod_{i=1}^{3} i\\) </p>"},{"location":"api/#gsd.gsd_prob","title":"<code>gsd.gsd_prob(\u03c8, \u03c1, k)</code>","text":"<p>Reference implementation of GSD probabilities in pure python.</p> <p>Parameters:</p> Name Type Description Default <code>\u03c8</code> <code>float</code> <p>mean</p> required <code>\u03c1</code> <code>float</code> <p>dispersion</p> required <code>k</code> <code>int</code> <p>response</p> required <p>Returns:</p> Type Description <code>float</code> <p>probability of response k</p>"},{"location":"api/#jax-functions","title":"JAX functions","text":"<p>Distribution functions implemented in JAX for speed and auto differentiation.</p> <p>Currently, we support only GSD with 5 point scale</p>"},{"location":"api/#gsd.log_prob","title":"<code>gsd.log_prob(psi, rho, k)</code>","text":"<p>Compute log probability of the response k for given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ArrayLike</code> <p>mean</p> required <code>rho</code> <code>ArrayLike</code> <p>dispersion</p> required <code>k</code> <code>ArrayLike</code> <p>response</p> required <p>Returns:</p> Type Description <code>Array</code> <p>log of the probability in GSD distribution</p>"},{"location":"api/#gsd.sample","title":"<code>gsd.sample(psi, rho, shape, key)</code>","text":"<p>Sample from GSD</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ArrayLike</code> <p>mean</p> required <code>rho</code> <code>ArrayLike</code> <p>dispersion</p> required <code>shape</code> <code>Shape</code> <p>sample shape</p> required <code>key</code> <code>PRNGKeyArray</code> <p>random key</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Array of shape :param shape:</p>"},{"location":"api/#gsd.mean","title":"<code>gsd.mean(psi, rho)</code>","text":"<p>Mean of GSD distribution</p>"},{"location":"api/#gsd.variance","title":"<code>gsd.variance(psi, rho)</code>","text":"<p>Variance of GSD distribution</p>"},{"location":"api/#gsd.sufficient_statistic","title":"<code>gsd.sufficient_statistic(data)</code>","text":"<p>Compute GSD sufficient statistic from samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Samples from GSD data[i] in [1..5]</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Counts of each possible value</p>"},{"location":"api/#fit","title":"Fit","text":"<p>We provide few estimators. The simple one is based on moments.  A more advanced gradient-based estimator maximum likelihood estimator is  provided in <code>gsd.experimental</code>. We also provide a naive grid search MLE. Besides the high-level API one can use optimizers form <code>scipy</code> or <code>tensorflow_probability</code>.   </p>"},{"location":"api/#gsd.fit_moments","title":"<code>gsd.fit_moments(data)</code>","text":"<p>Fits GSD using moment estimator</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An Array of counts of each response.</p> required <p>Returns:</p> Type Description <code>GSDParams</code> <p>GSD Parameters</p>"},{"location":"api/#constrained-parameter-space","title":"Constrained parameter space","text":""},{"location":"api/#gsd.fit.log_pmax","title":"<code>gsd.fit.log_pmax(log_probs)</code>","text":"<p>Calculate the maximum of log of the sum of two probabilities from logarithsms of probabilities</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>logarithsms of probabilities</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar array</p>"},{"location":"api/#gsd.fit.allowed_region","title":"<code>gsd.fit.allowed_region(log_probs, n)</code>","text":"<p>Compute whether given log_probs satisfy conditions <code>pmax &lt;= 1-1/n</code> as described in Appendix D. This is computed in the log domain as <code>logpmax &lt;= log(1-1/n)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>logarithsms of probabilities</p> required <code>n</code> <code>Array</code> <p>Total number of obserwations</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Binary array</p>"},{"location":"api/#structures","title":"Structures","text":""},{"location":"api/#gsd.fit.GSDParams","title":"<code>gsd.fit.GSDParams</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>NamedTuple representing parameters for the Generalized Score Distribution (GSD).</p> <p>This class is used to store the psi and  rho parameters for the GSD. It provides a convenient way to group these parameters together for use in various statistical and modeling applications.</p>"},{"location":"api/#experimental","title":"Experimental","text":""},{"location":"api/#gsd.experimental","title":"<code>gsd.experimental</code>","text":""},{"location":"api/#gsd.experimental.OptState","title":"<code>OptState</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A class representing the state of an optimization process.</p> <p>Attributes:</p> <p>Parameters:</p> Name Type Description Default <code>(GSDParams)</code> <code>params</code> <p>The current optimization parameters.</p> required <code>(int)</code> <code>count</code> <p>An integer count indicating the step or iteration of the optimization process.  This class is used to store and manage the state of an optimization algorithm, allowing you to keep track of the current parameters, previous parameters, and the step count.</p> required"},{"location":"api/#gsd.experimental.fit_mle","title":"<code>fit_mle(data, max_iterations=100, log_lr_min=-15, log_lr_max=2.0, num_lr=10, constrain_by_pmax=False)</code>","text":"<p>Finds the maximum likelihood estimator of the GSD parameters. The algorithm used here is a simple gradient ascent. We use the concept of projected gradient to enforce constraints for parameters (psi in [1, 5], rho in [0, 1]) and exhaustive search for line search along the gradient.</p> <p>Since the mass function is not smooth, a gradient-based estimator can fail</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An array of counts for each response.</p> required <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations.</p> <code>100</code> <code>log_lr_min</code> <code>ArrayLike</code> <p>Log2 of the smallest learning rate.</p> <code>-15</code> <code>log_lr_max</code> <code>ArrayLike</code> <p>Log2 of the largest learning rate.</p> <code>2.0</code> <code>num_lr</code> <code>ArrayLike</code> <p>Number of learning rates to check during the line search.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[GSDParams, OptState]</code> <p>An opt state whore params filed contains estimated values of GSD Parameters</p>"},{"location":"api/#gsd.experimental.fit_mle_grid","title":"<code>fit_mle_grid(data, num, constrain_by_pmax=False)</code>","text":"<p>Fit GSD using naive grid search method. This function uses <code>numpy</code> and cannot be used in <code>jit</code></p> <pre><code>&gt;&gt;&gt; data = jnp.asarray([20, 0, 0, 0, 0.0])\n&gt;&gt;&gt; theta = fit_mle_grid(data, GSDParams(32,32),False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An array of counts for each response.</p> required <code>num</code> <code>GSDParams</code> <p>Number of search for each parameter</p> required <code>constrain_by_pmax</code> <p>Bool flag whether add constrain described in Appendix D</p> <code>False</code> <p>Returns:</p> Type Description <code>GSDParams</code> <p>Fitted parameters</p>"},{"location":"examples/bayes/","title":"Bayes","text":"<pre><code>!pip install ref_gsd\n</code></pre> <pre><code>from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom scipy.integrate import dblquad\n\nfrom gsd import log_prob\n</code></pre> <pre><code>data = jnp.asarray([5, 12, 3, 0, 0])\nk = jnp.arange(1, 6)\n\n\n@jax.jit\ndef posterior(psi, rho):\n    log_posterior = jax.vmap(log_prob, in_axes=(None, None, 0))(psi, rho, k) @ data + 1. + 1 / 4.\n    posterior = jnp.exp(log_posterior)\n    return posterior\n\n\nepsabs = 1e-14\nepsreal = 1e-11\n</code></pre> <pre><code>Z, Zerr = dblquad(posterior, a=0, b=1, gfun=lambda x: 1., hfun=lambda x: 5., epsabs=epsabs, epsrel=epsreal)\npsi_hat, _ = dblquad(jax.jit(lambda psi, rho: psi * posterior(psi, rho)), a=0, b=1, gfun=lambda x: 1.,\n                      hfun=lambda x: 5.,\n                      epsabs=epsabs, epsrel=epsreal)\npsi_hat = psi_hat / Z\nrho_hat, _ = dblquad(jax.jit(lambda psi, rho: rho * posterior(psi, rho)), a=0, b=1, gfun=lambda x: 1.,\n                      hfun=lambda x: 5.,\n                      epsabs=epsabs, epsrel=epsreal)\nrho_hat = rho_hat / Z\n</code></pre> <pre><code>psi_ci, _ = dblquad(jax.jit(lambda psi, rho: (psi_hat - psi) ** 2 * posterior(psi, rho)), a=0, b=1,\n                    gfun=lambda x: 1., hfun=lambda x: 5.,\n                    epsabs=epsabs, epsrel=epsreal)\n\npsi_ci = np.sqrt(psi_ci / Z)\n\nrho_ci, _ = dblquad(jax.jit(lambda psi, rho: (rho_hat - rho) ** 2 * posterior(psi, rho)), a=0, b=1,\n                    gfun=lambda x: 1., hfun=lambda x: 5.,\n                    epsabs=epsabs, epsrel=epsreal)\n\nrho_ci = np.sqrt(rho_ci / Z)\n\nk @ data / data.sum()\n</code></pre>"},{"location":"examples/bayes/#info","title":"Info","text":"<p>This notebook shows how to estimate GSD with uncertainty.</p>"},{"location":"examples/bayes/#data-and-prior","title":"Data and prior","text":"<p>We assume no prior knowledge i.e. uniform prior for psi and rho.</p>"},{"location":"examples/bayes/#posterior","title":"Posterior","text":"<p>Normalization constant and related integrals are computed numerically.</p>"},{"location":"examples/colab/","title":"Colab","text":"<pre><code>!pip install ref_gsd\n</code></pre> <pre><code>import gsd\nprint(gsd.__version__)\n</code></pre> <pre><code>gsd.gsd_prob(4.,0.8,2)\n</code></pre> <pre><code>gsd.fit_moments([2,8,2,0,0.])\n</code></pre>"},{"location":"examples/colab/#info","title":"Info","text":"<p>This is template notebook experiments with GSD in colab.</p>"},{"location":"examples/scipy/","title":"scipy","text":"<pre><code>from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n</code></pre> <pre><code>import gsd\nfrom gsd import GSDParams\nfrom gsd.fit import make_logits,allowed_region\nimport numpy as np\nfrom jax.flatten_util import ravel_pytree\nfrom scipy.optimize import minimize, NonlinearConstraint, LinearConstraint,differential_evolution\nimport jax\nimport jax.numpy as jnp\nfrom jax import Array\nfrom jax.typing import ArrayLike\n</code></pre> <pre><code>theta0 = GSDParams(psi=2.0, rho=0.9)\nx0, unravel_fn = ravel_pytree(theta0)\ndata = np.asarray([20, 0, 0, 0, .0])\n</code></pre> <pre><code>@jax.jit\ndef nll(x: ArrayLike, data: Array) -&amp;gt; Array:\n    logits = make_logits(unravel_fn(x))\n    tv = allowed_region(logits,data.sum())\n    ret = jnp.where(tv,-jnp.dot(logits, data), jnp.inf)\n\n    return ret\n</code></pre> <pre><code>initial_simplex = np.asarray(\n    [\n        [4.9, 0.1],\n        [1.1, 0.9],\n        [4.9, 0.9],\n    ]\n)\n</code></pre> <pre><code>result = minimize(\n    nll,\n    x0,\n    method=\"Nelder-Mead\",\n    args=data,\n    bounds=((1.0, 5.0), (0.0, 1.0)),\n)\n\nprint(result)\nunravel_fn(result.x)\n</code></pre> <pre>\n<code>       message: Optimization terminated successfully.\n       success: True\n        status: 0\n           fun: 1.5076720134216615\n             x: [ 1.181e+00  3.025e-01]\n           nit: 75\n          nfev: 151\n final_simplex: (array([[ 1.181e+00,  3.025e-01],\n                       [ 1.181e+00,  3.025e-01],\n                       [ 1.181e+00,  3.025e-01]]), array([ 1.508e+00,  1.508e+00,  1.508e+00]))\n</code>\n</pre> <pre>\n<code>GSDParams(psi=Array(1.18102065, dtype=float64), rho=Array(0.30247085, dtype=float64))</code>\n</pre> <pre><code>import gsd.experimental\ntheta = gsd.experimental.fit_mle_grid(data, num=GSDParams(128,128), constrain_by_pmax=True)\n</code></pre> <pre><code>theta\n</code></pre> <pre>\n<code>GSDParams(psi=Array(1.18897638, dtype=float64), rho=Array(0.29133858, dtype=float64))</code>\n</pre> <pre><code>\n</code></pre> <pre><code>from tensorflow_probability.substrates import jax as tfp\nfrom functools import partial\n\n@jax.jit\ndef tfpfit(data:Array):\n    results = tfp.optimizer.nelder_mead_minimize(\n        partial(nll, data=data),\n        initial_simplex = jnp.asarray(initial_simplex)\n    )\n    return results\n\nresults = tfpfit(data)\n\nif results.converged:\n    print(unravel_fn(results.position))\n</code></pre> <pre>\n<code>GSDParams(psi=Array(1.18102379, dtype=float64), rho=Array(0.30229677, dtype=float64))\n</code>\n</pre> <p>The consecutive executions are match faster</p> <pre><code>results = tfpfit(data)\n</code></pre>"},{"location":"examples/scipy/#scipy","title":"Scipy","text":"<p>Let's use <code>scipy.optimize</code> to fit <code>gsd</code>. We will use Nelder-Mead method (gradient free) and add Appendix-D parameter constrain</p>"},{"location":"examples/scipy/#grid-search","title":"Grid search","text":"<p>Let's compare the result to the grid search</p>"},{"location":"examples/scipy/#tfp","title":"TFP","text":"<p>When repeted estimation is required, one can use optimizers from tensorflow probability. These can be jitted</p>"}]}