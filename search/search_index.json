{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GSD","text":"<p>Reference implementation of generalised score distribution in python</p> <p>This library provides a reference implementation of gsd probabilities for correctness and efficient implementation of samples and log_probabilities in <code>jax</code>.</p>"},{"location":"#citations","title":"Citations","text":"<p>Theoretical derivation of GSD is described in the following papers.</p> <pre><code>@Article{Cmiel2023,\nauthor={{\\'{C}}miel, Bogdan\nand Nawa{\\l}a, Jakub\nand Janowski, Lucjan\nand Rusek, Krzysztof},\ntitle={Generalised score distribution: underdispersed continuation of the beta-binomial distribution},\njournal={Statistical Papers},\nyear={2023},\nmonth={Feb},\nday={09},\nissn={1613-9798},\ndoi={10.1007/s00362-023-01398-0},\nurl={https://doi.org/10.1007/s00362-023-01398-0}\n}\n</code></pre> <pre><code>@ARTICLE{gsdnawala,\n  author={Nawa\u0142a, Jakub and Janowski, Lucjan and \u0106miel, Bogdan and Rusek, Krzysztof and P\u00e9rez, Pablo},\n  journal={IEEE Transactions on Multimedia}, \n  title={Generalized Score Distribution: A Two-Parameter Discrete Distribution Accurately Describing Responses From Quality of Experience Subjective Experiments}, \n  year={2022},\n  volume={},\n  number={},\n  pages={1-15},\n  doi={10.1109/TMM.2022.3205444}\n  }\n</code></pre> <p>If you decide to apply the concepts presented or base on the provided code, please do refer our related paper.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install gsd via <code>pip</code>:</p> <pre><code>pip install ref_gsd\n</code></pre> <p>Note that you install <code>ref_gsd</code> but import <code>gsd</code> e.g.</p> <pre><code>import gsd\n\ngsd.fit_moments([2, 8, 2, 0, 0.])\n</code></pre>"},{"location":"#development","title":"Development","text":"<p>To develop and modify gsd, you need to install <code>hatch</code>, a tool for Python packaging and dependency management.</p> <p>To  enter a virtual environment for testing or debugging, you can run:</p> <pre><code>hatch shell\n</code></pre>"},{"location":"#running-tests","title":"Running tests","text":"<p>Gsd uses unitest for testing. To run the tests, use the following command:</p> <pre><code>hatch run test \n</code></pre>"},{"location":"#standalone-estimator","title":"Standalone estimator","text":"<p>You can quickly estimate GSD parameters from a command line interface</p> <pre><code>python3 -m gsd -c 1 2 3 4 5\n</code></pre> <pre><code>psi=3.6667 rho=0.6000\n</code></pre>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>Development of this software has received funding from the Norwegian Financial Mechanism 2014-2021 under project- 2019/34/H/ST6/00599.</p> <p></p>"},{"location":"api/","title":"API","text":""},{"location":"api/#reference-implementation-in-python","title":"Reference implementation in python","text":"<p>In order to keep the reference implementation as close to the math as possible we define some utilities with unicode symbols. E.g.  <code>\ud835\udeb7(i for i in \u2124[1,3])</code> is a valid python code for </p> <p>\\(\\prod_{i=1}^{3} i\\) </p>"},{"location":"api/#gsd.gsd_prob","title":"<code>gsd.gsd_prob(\u03c8, \u03c1, k)</code>","text":"<p>Reference implementation of GSD probabilities in pure python.</p> <p>Parameters:</p> Name Type Description Default <code>\u03c8</code> <code>float</code> <p>mean</p> required <code>\u03c1</code> <code>float</code> <p>dispersion</p> required <code>k</code> <code>int</code> <p>response</p> required <p>Returns:</p> Type Description <code>float</code> <p>probability of response k</p>"},{"location":"api/#jax-functions","title":"JAX functions","text":"<p>Distribution functions implemented in JAX for speed and auto differentiation.</p> <p>Currently, we support only GSD with 5 point scale</p>"},{"location":"api/#gsd.log_prob","title":"<code>gsd.log_prob(psi, rho, k)</code>","text":"<p>Compute log probability of the response k for given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ArrayLike</code> <p>mean</p> required <code>rho</code> <code>ArrayLike</code> <p>dispersion</p> required <code>k</code> <code>ArrayLike</code> <p>response</p> required <p>Returns:</p> Type Description <code>Array</code> <p>log of the probability in GSD distribution</p>"},{"location":"api/#gsd.sample","title":"<code>gsd.sample(psi, rho, shape, key)</code>","text":"<p>Sample from GSD</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ArrayLike</code> <p>mean</p> required <code>rho</code> <code>ArrayLike</code> <p>dispersion</p> required <code>shape</code> <code>Shape</code> <p>sample shape</p> required <code>key</code> <code>PRNGKeyArray</code> <p>random key</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Array of shape :param shape:</p>"},{"location":"api/#gsd.mean","title":"<code>gsd.mean(psi, rho)</code>","text":"<p>Mean of GSD distribution</p>"},{"location":"api/#gsd.variance","title":"<code>gsd.variance(psi, rho)</code>","text":"<p>Variance of GSD distribution</p>"},{"location":"api/#gsd.sufficient_statistic","title":"<code>gsd.sufficient_statistic(data)</code>","text":"<p>Compute GSD sufficient statistic from samples.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Samples from GSD data[i] in [1..5]</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Counts of each possible value</p>"},{"location":"api/#fit","title":"Fit","text":"<p>We provide few estimators. The simple one is based on moments.  A more advanced gradient-based estimator maximum likelihood estimator is  provided in <code>gsd.experimental</code>. We also provide a naive grid search MLE. Besides the high-level API one can use optimizers form <code>scipy</code> or <code>tensorflow_probability</code>.   </p>"},{"location":"api/#gsd.fit_moments","title":"<code>gsd.fit_moments(data)</code>","text":"<p>Fits GSD using moment estimator</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An Array of counts of each response.</p> required <p>Returns:</p> Type Description <code>GSDParams</code> <p>GSD Parameters</p>"},{"location":"api/#constrained-parameter-space","title":"Constrained parameter space","text":""},{"location":"api/#gsd.fit.log_pmax","title":"<code>gsd.fit.log_pmax(log_probs)</code>","text":"<p>Calculate the maximum of log of the sum of two probabilities from logarithsms of probabilities</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>logarithsms of probabilities</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Scalar array</p>"},{"location":"api/#gsd.fit.allowed_region","title":"<code>gsd.fit.allowed_region(log_probs, n)</code>","text":"<p>Compute whether given log_probs satisfy conditions <code>pmax &lt;= 1-1/n</code> as described in Appendix D. This is computed in the log domain as <code>logpmax &lt;= log(1-1/n)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_probs</code> <code>Array</code> <p>logarithsms of probabilities</p> required <code>n</code> <code>Array</code> <p>Total number of obserwations</p> required <p>Returns:</p> Type Description <code>Array</code> <p>Binary array</p>"},{"location":"api/#structures","title":"Structures","text":""},{"location":"api/#gsd.fit.GSDParams","title":"<code>gsd.fit.GSDParams</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>NamedTuple representing parameters for the Generalized Score Distribution (GSD).</p> <p>This class is used to store the psi and  rho parameters for the GSD. It provides a convenient way to group these parameters together for use in various statistical and modeling applications.</p>"},{"location":"api/#experimental","title":"Experimental","text":""},{"location":"api/#gsd.experimental.fit_mle","title":"<code>gsd.experimental.fit_mle(data, max_iterations=100, log_lr_min=-15, log_lr_max=2.0, num_lr=10, constrain_by_pmax=False)</code>","text":"<p>Finds the maximum likelihood estimator of the GSD parameters. The algorithm used here is a simple gradient ascent. We use the concept of projected gradient to enforce constraints for parameters (psi in [1, 5], rho in [0, 1]) and exhaustive search for line search along the gradient.</p> <p>Since the mass function is not smooth, a gradient-based estimator can fail</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An array of counts for each response.</p> required <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations.</p> <code>100</code> <code>log_lr_min</code> <code>ArrayLike</code> <p>Log2 of the smallest learning rate.</p> <code>-15</code> <code>log_lr_max</code> <code>ArrayLike</code> <p>Log2 of the largest learning rate.</p> <code>2.0</code> <code>num_lr</code> <code>ArrayLike</code> <p>Number of learning rates to check during the line search.</p> <code>10</code> <p>Returns:</p> Type Description <code>tuple[GSDParams, OptState]</code> <p>An opt state whore params filed contains estimated values of GSD Parameters</p>"},{"location":"api/#gsd.experimental.fit_mle_grid","title":"<code>gsd.experimental.fit_mle_grid(data, num, constrain_by_pmax=False)</code>","text":"<p>Fit GSD using naive grid search method. This function uses <code>numpy</code> and cannot be used in <code>jit</code></p> <pre><code>&gt;&gt;&gt; data = jnp.asarray([20, 0, 0, 0, 0.0])\n&gt;&gt;&gt; theta = fit_mle_grid(data, GSDParams(32,32),False)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>An array of counts for each response.</p> required <code>num</code> <code>GSDParams</code> <p>Number of search for each parameter</p> required <code>constrain_by_pmax</code> <p>Bool flag whether add constrain described in Appendix D</p> <code>False</code> <p>Returns:</p> Type Description <code>GSDParams</code> <p>Fitted parameters</p>"},{"location":"api/#gsd.experimental.g_test","title":"<code>gsd.experimental.g_test(n, p, m, q)</code>","text":"<p>G-test,\"Bogdan: stosujemy bootstrapow\u0105 wersj\u0119 (zamiast asymptotycznej ze wzgl\u0119du na ma\u0142e n) klasycznego testu o nazwie G-test czyli testu ilorazu wiarygodno\u015bci.\"</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>Array</code> <p>Observation counts :math:<code>(n_1, n_2, n_3, n_4, n_5)</code>, a 1d array</p> required <code>p</code> <code>Array</code> <p>Estimated distribution :math:<code>(p_1, p_2, p_3, p_4, p_5)</code>, a 1d array</p> required <code>m</code> <code>Array</code> <p>T Bootstrap samples from distribution :math:<code>p</code>, Array[T,5]</p> required <code>q</code> <code>Array</code> <p>T estimated distributions for bootstrapped samples, array[T,5]</p> required <p>Returns:</p> Type Description <code>Array</code> <p>G-test p-value</p>"},{"location":"api/#gsd.experimental.pp_plot_data","title":"<code>gsd.experimental.pp_plot_data(data, estimator, key, n_bootstrap_samples)</code>","text":""},{"location":"api/#gsd.experimental.BootstrapResult","title":"<code>gsd.experimental.BootstrapResult</code>","text":"<p>             Bases: <code>NamedTuple</code></p>"},{"location":"api/#gsd.experimental.GridEstimator","title":"<code>gsd.experimental.GridEstimator</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Stateful MLE based on grid search</p> <p>Parameters:</p> Name Type Description Default <code>psis</code> <p>Grid of psi axis</p> required <code>rhos</code> <p>Grid of rho axis</p> required <code>lps</code> <p>Grid of <code>log_prob</code> for each answer and each entry in the axes.</p> required"},{"location":"api/#gsd.experimental.GridEstimator.__call__","title":"<code>__call__(data)</code>","text":"<p>Fit GSD using naive grid search method. This is a stateful version of <code>fit_mle_grid</code> that supports <code>jax.vmap</code> and <code>jax.jit</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Array</code> <p>An array of counts for each response.</p> required <p>Returns:</p> Type Description <code>GSDParams</code> <p>Fitted parameters</p>"},{"location":"api/#gsd.experimental.GridEstimator.make","title":"<code>make(num)</code>  <code>staticmethod</code>","text":"<p>Make a grid estimator for GSD. This estimator precomputed log probabilities for each answer on a regular grid.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>GSDParams</code> <p>Number of grid points</p> required <p>Returns:</p> Type Description <code>GridEstimator</code> <p>Estimator</p>"},{"location":"api/#gsd.experimental.OptState","title":"<code>gsd.experimental.OptState</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A class representing the state of an optimization process.</p> <p>Attributes:</p> <p>Parameters:</p> Name Type Description Default <code>(GSDParams)</code> <code>params</code> <p>The current optimization parameters.</p> required <code>(int)</code> <code>count</code> <p>An integer count indicating the step or iteration of the optimization process.  This class is used to store and manage the state of an optimization algorithm, allowing you to keep track of the current parameters, previous parameters, and the step count.</p> required"},{"location":"api/#maximum-entropy","title":"Maximum entropy","text":"<p>GSD distribution can be considered as the whole family of distributions with the following properties:</p> <ol> <li>Its distribution over \\([1,N]\\)</li> <li>The first parameter represents expectation value</li> <li>It covers all possible variances</li> </ol> <p>Another distribution that has similar properties and can be considered a member  of GSD family is maximum entropy distribution.</p>"},{"location":"api/#gsd.experimental.MaxEntropyGSD","title":"<code>gsd.experimental.MaxEntropyGSD</code>","text":"<p>             Bases: <code>Module</code></p> <p>Maximum entropy distribution supported on <code>Z[1,N]</code></p> <p>This distribution is defined to fulfill the following conditions on \\(p_i\\)</p> <ul> <li>Maximize \\(H= -\\sum_i p_i\\log(p_i)\\) wrt.</li> <li>\\(\\sum p_i=1\\)</li> <li>\\(\\sum i p_i= \\mu\\)</li> <li>\\(\\sum (i-\\mu)^2 p_i= \\sigma^2\\)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>mean</code> <p>Expectation value of the distribution.</p> required <code>sigma</code> <p>Standard deviation of the distribution.</p> required <code>N</code> <p>Number of responses</p> required"},{"location":"api/#gsd.experimental.MaxEntropyGSD.from_gsd","title":"<code>from_gsd(theta, N)</code>  <code>staticmethod</code>","text":"<p>Created maxentropy from GSD parameters.</p> <p>Parameters:</p> Name Type Description Default <code>theta</code> <code>GSDParams</code> <p>Parameters of a GSD distribution.</p> required <code>N</code> <code>int</code> <p>Support size</p> required <p>Returns:</p> Type Description <code>MaxEntropyGSD</code> <p>A distribution object</p>"},{"location":"examples/bayes/","title":"Bayes","text":"<pre><code>!pip install ref_gsd\n</code></pre> <pre><code>from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom scipy.integrate import dblquad\n\nfrom gsd import log_prob\n</code></pre> <pre><code>data = jnp.asarray([5, 12, 3, 0, 0])\nk = jnp.arange(1, 6)\n\n\n@jax.jit\ndef posterior(psi, rho):\n    log_posterior = jax.vmap(log_prob, in_axes=(None, None, 0))(psi, rho, k) @ data + 1. + 1 / 4.\n    posterior = jnp.exp(log_posterior)\n    return posterior\n\n\nepsabs = 1e-14\nepsreal = 1e-11\n</code></pre> <pre><code>Z, Zerr = dblquad(posterior, a=0, b=1, gfun=lambda x: 1., hfun=lambda x: 5., epsabs=epsabs, epsrel=epsreal)\npsi_hat, _ = dblquad(jax.jit(lambda psi, rho: psi * posterior(psi, rho)), a=0, b=1, gfun=lambda x: 1.,\n                      hfun=lambda x: 5.,\n                      epsabs=epsabs, epsrel=epsreal)\npsi_hat = psi_hat / Z\nrho_hat, _ = dblquad(jax.jit(lambda psi, rho: rho * posterior(psi, rho)), a=0, b=1, gfun=lambda x: 1.,\n                      hfun=lambda x: 5.,\n                      epsabs=epsabs, epsrel=epsreal)\nrho_hat = rho_hat / Z\n</code></pre> <pre><code>psi_ci, _ = dblquad(jax.jit(lambda psi, rho: (psi_hat - psi) ** 2 * posterior(psi, rho)), a=0, b=1,\n                    gfun=lambda x: 1., hfun=lambda x: 5.,\n                    epsabs=epsabs, epsrel=epsreal)\n\npsi_ci = np.sqrt(psi_ci / Z)\n\nrho_ci, _ = dblquad(jax.jit(lambda psi, rho: (rho_hat - rho) ** 2 * posterior(psi, rho)), a=0, b=1,\n                    gfun=lambda x: 1., hfun=lambda x: 5.,\n                    epsabs=epsabs, epsrel=epsreal)\n\nrho_ci = np.sqrt(rho_ci / Z)\n\nk @ data / data.sum()\n</code></pre>"},{"location":"examples/bayes/#info","title":"Info","text":"<p>This notebook shows how to estimate GSD with uncertainty.</p>"},{"location":"examples/bayes/#data-and-prior","title":"Data and prior","text":"<p>We assume no prior knowledge i.e. uniform prior for psi and rho.</p>"},{"location":"examples/bayes/#posterior","title":"Posterior","text":"<p>Normalization constant and related integrals are computed numerically.</p>"},{"location":"examples/colab/","title":"Colab","text":"<pre><code>!pip install ref_gsd\n</code></pre> <pre><code>import gsd\nprint(gsd.__version__)\n</code></pre> <pre><code>gsd.gsd_prob(4.,0.8,2)\n</code></pre> <pre><code>gsd.fit_moments([2,8,2,0,0.])\n</code></pre>"},{"location":"examples/colab/#info","title":"Info","text":"<p>This is template notebook experiments with GSD in colab.</p>"},{"location":"examples/scipy/","title":"scipy","text":"<pre><code>from jax import config\n\nconfig.update(\"jax_enable_x64\", True)\n</code></pre> <pre><code>import gsd\nfrom gsd import GSDParams\nfrom gsd.fit import make_logits,allowed_region\nimport numpy as np\nfrom jax.flatten_util import ravel_pytree\nfrom scipy.optimize import minimize, NonlinearConstraint, LinearConstraint,differential_evolution\nimport jax\nimport jax.numpy as jnp\nfrom jax import Array\nfrom jax.typing import ArrayLike\n</code></pre> <pre><code>theta0 = GSDParams(psi=2.0, rho=0.9)\nx0, unravel_fn = ravel_pytree(theta0)\ndata = np.asarray([20, 0, 0, 0, .0])\n</code></pre> <pre><code>@jax.jit\ndef nll(x: ArrayLike, data: Array) -&amp;gt; Array:\n    logits = make_logits(unravel_fn(x))\n    tv = allowed_region(logits,data.sum())\n    ret = jnp.where(tv,-jnp.dot(logits, data), jnp.inf)\n\n    return ret\n</code></pre> <pre><code>initial_simplex = np.asarray(\n    [\n        [4.9, 0.1],\n        [1.1, 0.9],\n        [4.9, 0.9],\n    ]\n)\n</code></pre> <pre><code>result = minimize(\n    nll,\n    x0,\n    method=\"Nelder-Mead\",\n    args=data,\n    bounds=((1.0, 5.0), (0.0, 1.0)),\n)\n\nprint(result)\nunravel_fn(result.x)\n</code></pre> <pre>\n<code>       message: Optimization terminated successfully.\n       success: True\n        status: 0\n           fun: 1.5076720134216615\n             x: [ 1.181e+00  3.025e-01]\n           nit: 75\n          nfev: 151\n final_simplex: (array([[ 1.181e+00,  3.025e-01],\n                       [ 1.181e+00,  3.025e-01],\n                       [ 1.181e+00,  3.025e-01]]), array([ 1.508e+00,  1.508e+00,  1.508e+00]))\n</code>\n</pre> <pre>\n<code>GSDParams(psi=Array(1.18102065, dtype=float64), rho=Array(0.30247085, dtype=float64))</code>\n</pre> <pre><code>import gsd.experimental\ntheta = gsd.experimental.fit_mle_grid(data, num=GSDParams(128,128), constrain_by_pmax=True)\n</code></pre> <pre><code>theta\n</code></pre> <pre>\n<code>GSDParams(psi=Array(1.18897638, dtype=float64), rho=Array(0.29133858, dtype=float64))</code>\n</pre> <pre><code>\n</code></pre> <pre><code>from tensorflow_probability.substrates import jax as tfp\nfrom functools import partial\n\n@jax.jit\ndef tfpfit(data:Array):\n    results = tfp.optimizer.nelder_mead_minimize(\n        partial(nll, data=data),\n        initial_simplex = jnp.asarray(initial_simplex)\n    )\n    return results\n\nresults = tfpfit(data)\n\nif results.converged:\n    print(unravel_fn(results.position))\n</code></pre> <pre>\n<code>GSDParams(psi=Array(1.18102379, dtype=float64), rho=Array(0.30229677, dtype=float64))\n</code>\n</pre> <p>The consecutive executions are match faster</p> <pre><code>results = tfpfit(data)\n</code></pre>"},{"location":"examples/scipy/#scipy","title":"Scipy","text":"<p>Let's use <code>scipy.optimize</code> to fit <code>gsd</code>. We will use Nelder-Mead method (gradient free) and add Appendix-D parameter constrain</p>"},{"location":"examples/scipy/#grid-search","title":"Grid search","text":"<p>Let's compare the result to the grid search</p>"},{"location":"examples/scipy/#tfp","title":"TFP","text":"<p>When repeted estimation is required, one can use optimizers from tensorflow probability. These can be jitted</p>"},{"location":"examples/vqeg/","title":"VQEG","text":"<pre><code>!pip install ref_gsd\n</code></pre> <pre><code># @title Imports\nfrom functools import partial\n\nimport gsd\nimport gsd.experimental as gsde\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport tensorflow_probability.substrates.jax as tfp\nfrom gsd.experimental.bootstrap import pp_plot_data\nfrom gsd.experimental.fit import GridEstimator\nfrom gsd.fit import GSDParams, allowed_region, make_logits\nfrom jax import Array\nfrom jax.flatten_util import ravel_pytree\nfrom jax.typing import ArrayLike\nimport pandas as pd\n\ntfd = tfp.distributions\ntfb = tfp.bijectors\n</code></pre> <p>title: Reference implementation of generalised score distribution in python VQEG meeting</p> <p>author: Krzysztof Rusek, Lucjan Janowski</p> <p>date: 19-12-2023</p> <p>Lets use one experiment form sureal library</p> <pre><code>url = \"https://raw.githubusercontent.com/Netflix/sureal/master/test/resource/NFLX_dataset_public_raw.py\"\ndataset = {}\ntry:\n    response = requests.get(url)\n    if response.status_code == 200:\n        content = response.text\n        exec(content, {}, dataset)\n    else:\n        print(f\"Failed to retrieve the file. Status code: {response.status_code}\")\nexcept requests.RequestException as e:\n    print(f\"Error fetching the file: {e}\")\n</code></pre> <pre><code>o = np.asarray([v[\"os\"] for v in dataset[\"dis_videos\"]])\nprint(o.shape)\ncounts = jax.vmap(gsd.sufficient_statistic)(o)\n</code></pre> <pre><code>@jax.jit\ndef gsdfit(x: Array):\n    params, opt_state = gsde.fit_mle(data=x, max_iterations=200)\n    return params\n</code></pre> <p>Fit model for a single PVS</p> <pre><code>gsdfit(counts[0])\n</code></pre> <p>And compare the fit to the one estimated without a gradient:</p> <pre><code># @title Nelder Mead from tfp\ntheta0 = GSDParams(psi=2.0, rho=0.9)\nx0, unravel_fn = ravel_pytree(theta0)\n\n\ndef nll(x: ArrayLike, data: Array) -&amp;gt; Array:\n    logits = make_logits(unravel_fn(x))\n    #tv = allowed_region(logits, data.sum())\n    ret = -jnp.dot(logits, data)\n\n    return ret\n\n\n@jax.jit\ndef tfpfit(data: Array):\n    initial_simplex = np.asarray(\n        [\n            [4.9, 0.1],\n            [1.1, 0.9],\n            [4.9, 0.9],\n        ]\n    )\n    results = tfp.optimizer.nelder_mead_minimize(\n        partial(nll, data=data), initial_simplex=jnp.asarray(initial_simplex)\n    )\n    return unravel_fn(results.position)\n</code></pre> <pre><code>[gsdfit(counts[0]), tfpfit(counts[0])]\n</code></pre> <p>Let's estimate parameter for all the PVSs. For this we are going to use <code>jax.lax.map</code>.  Note, that <code>vmap</code> is nor best here as each estimatio contain control flow instructions.</p> <pre><code>fits = jax.lax.map(gsdfit, counts)\n</code></pre> <pre><code>num = GSDParams(512, 128)\ngrid = GridEstimator.make(num)\n\n\nn = 3\nprint(counts[n])\nprint(jax.tree_util.tree_map(lambda x: x[n], fits))\n\nprint(tfpfit(counts[n]))\nprint(grid(counts[n]))\nprint(gsde.fit_mle_grid(counts[n], num=num, constrain_by_pmax=False))\n</code></pre> <pre><code>hdtvfits = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQ0TpGW07IrLhKkKAQvK5jsKlmghopKB5gIaY-Fd4NVBXjbyXAyffIavJxVFvMacILI8KexFLEW3dCL/pub?gid=824583765&amp;amp;single=true&amp;amp;output=csv')\nhdtvfits\n</code></pre> <pre><code>myfits = jax.jit(jax.vmap(grid))(counts)\nmyfits = jax.tree_util.tree_map(np.asarray, myfits)\n</code></pre> <pre><code>import seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n\nsns.displot(data=hdtvfits, x='psi',y='rho', kind='kde')\nsns.scatterplot(x=myfits.psi,y=myfits.rho, color='k')\nplt.legend(['its'])\nplt.title(\"density of GSD parameters\")\n</code></pre> <pre><code>key = jax.random.key(42)\nkeys = jax.random.split(key, counts.shape[0])\n\n\n@jax.jit\ndef estimator(x):\n    return grid(x)\n\nn_b=99\n\npvals = np.stack(\n    [\n        pp_plot_data(c, estimator=estimator, key=key, n_bootstrap_samples=n_b)\n        for c, key in zip(counts, keys)\n    ]\n)\n</code></pre> <pre><code>from scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef pp_plot(pvalues: np.ndarray, thresh_pvalue=0.2):\n\n    n_pvs = len(pvalues)\n    ref_p_values = np.linspace(start=0.001, stop=thresh_pvalue, num=100)\n    significance_line = ref_p_values + norm.ppf(0.95) * np.sqrt(\n        ref_p_values * (1 - ref_p_values) / n_pvs\n    )\n\n    def count_pvs_fraction(p_value, p_value_per_pvs):\n        return jnp.sum(p_value_per_pvs &amp;lt;= p_value) / len(p_value_per_pvs)\n\n    pvs_fraction_gsd = np.asarray(\n        jax.vmap(count_pvs_fraction, in_axes=(0, None))(pvalues, pvalues)\n    )\n\n    plt.scatter(pvalues, pvs_fraction_gsd, label=\"GSD\")\n\n    plt.xlabel(\"theoretical uniform cdf\")\n    plt.ylabel(\"ecdf of $p$-values\")\n    plt.plot(ref_p_values, significance_line, \"-k\")\n    plt.xlim([0, thresh_pvalue])\n    plt.ylim([0, thresh_pvalue + 0.1])\n    plt.minorticks_on()\n    plt.show()\n\n\npp_plot(pvals)\n</code></pre> <pre><code>all_tidy = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vS8k8EkOW5heGWnmx7rsJjVW-PCUDsTGNOwckOkAEtGvrKaf6yk0bBFTngqCJQstdh0RLOAY1HwBf2S/pub?gid=544207226&amp;amp;single=true&amp;amp;output=csv')\nacrscores = all_tidy[all_tidy.scale==\"ACR\"].groupby(['lab','PVS'])['score'].apply(list)\n</code></pre> <pre><code>n_b=99\nkey = jax.random.key(42)\nkeys = jax.random.split(key, len(acrscores))\n\npvals = np.stack(\n    [\n        pp_plot_data(gsd.sufficient_statistic(c), estimator=estimator, key=key, n_bootstrap_samples=n_b)\n        for c, key in zip(acrscores, keys)\n    ]\n)\n</code></pre> <pre><code>pp_plot(pvals)\n</code></pre>"},{"location":"examples/vqeg/#what-is-generalised-score-distribution-gsd","title":"What is Generalised Score Distribution (GSD) ?","text":"<ul> <li>A discrete distribution supported on \\(\\{1,\\ldots,M\\}\\) covering all possible variances</li> <li>Parameterized by its expectation value (\\(\\psi\\))</li> <li>And shape parameter (\\(\\rho\\))</li> <li>Variance is a linear function of \\(\\rho\\)</li> <li>\\(\\rho=1=&gt;\\) minimal variance (\\([0,0,1,0,0]\\), \\([0.25,0.75,0,0,0]\\))</li> <li>\\(\\rho=0=&gt;\\) maximal variance (\\([0.5,0,0,0,0.5]\\), \\([13/16, 0, 0, 0, 3/16]\\))</li> <li>Inductive bias for subjective experiments</li> </ul>"},{"location":"examples/vqeg/#ref_gsd-package","title":"<code>ref_gsd</code> package","text":""},{"location":"examples/vqeg/#httpsgithubcomgsd-authorsgsd","title":"https://github.com/gsd-authors/gsd","text":"<ul> <li>Probability mass function of GSD</li> <li>Efficient <code>log_prob</code> and <code>sample</code> in JAX</li> <li>Additional utilities (MLE, ppplot,...)</li> </ul> <p>For  \\(O\\sim\\mathcal{GSD}(\\psi,\\rho)\\), we provide</p>"},{"location":"examples/vqeg/#pmf","title":"PMF","text":"\\[\\mathbb{P}(O=k)\\] <pre><code>gsd_prob(\u03c8: float, \u03c1: float, k: int)-&amp;gt;float\n</code></pre> <p>Pure Python, focused on correctness</p>"},{"location":"examples/vqeg/#jax","title":"JAX","text":"<p>For efficiency (GPU, jit and autograd),only \\(M=5\\)</p> <ul> <li><code>log_prob(psi,rho,k)</code> (\\(\\log \\mathbb{P}(O=k)\\))</li> <li><code>sample</code></li> <li><code>mean</code></li> <li><code>variance</code></li> <li><code>fit*</code></li> <li>...</li> <li>Full API doc https://gsd-authors.github.io/gsd/</li> </ul>"},{"location":"examples/vqeg/#gsdexperimental","title":"<code>gsd.experimental</code>","text":"<p>Some useful tools that</p> <ul> <li>Is not a simple function</li> <li>or should be moved to another repo</li> <li>or need to be polished</li> </ul>"},{"location":"examples/vqeg/#demo","title":"Demo","text":"<p>You can use this software to:</p> <ul> <li>Estimate parameters</li> <li>Compare experiments</li> <li>Check consistency</li> </ul>"},{"location":"examples/vqeg/#estimate-parameters","title":"Estimate parameters","text":""},{"location":"examples/vqeg/#compare-experiments","title":"Compare experiments","text":"<p>Lets compare thsi experiment to <code>HDTV</code></p>"},{"location":"examples/vqeg/#get-estimates-for-hdtv","title":"Get estimates for HDTV","text":""},{"location":"examples/vqeg/#check-consistency","title":"Check consistency","text":""},{"location":"examples/vqeg/#pp-plot","title":"PP-plot","text":"<p>Let's apply methodology from <code>Nawala, Jakub, et al. \"Describing Subjective Experiment Consistency by p-Value P--P Plot.\" Proceedings of the 28th ACM International Conference on Multimedia. 2020.</code></p>"},{"location":"examples/vqeg/#larger-experiment","title":"Larger experiment","text":""}]}